- Triton inference server: Inflight batching
- Speculative decoding
- Model optimizer: Sparsity (Nvidia 2:4 Sparsity)
- TensorRT-LLM
- Tensor Parallelism
