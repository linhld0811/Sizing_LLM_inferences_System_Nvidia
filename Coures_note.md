- Triton inference server: Inflight batching
- Speculative decoding
- Model optimizer: Sparsity (Nvidia 2:4 Sparsity)
- TensorRT-LLM
- Tensor Parallelism


# 
- [Genai-perf](https://github.com/triton-inference-server/client/tree/r24.03/src/c++/perf_analyzer/genai-perf)
